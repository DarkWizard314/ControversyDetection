{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28090\n",
      "3798\n",
      "3798\n",
      "3798\n",
      "\n",
      "Topics by Latent Dirichlet Allocation model\n",
      "Topic #1 (66, '0.006*\"book\" + 0.005*\"Rick\" + 0.004*\"human\" + 0.004*\"aleve\" + 0.004*\"choose\" + 0.004*\"heartburn\" + 0.004*\"choice\" + 0.004*\"crime\" + 0.003*\"eye\" + 0.003*\"Tanya\"')\n",
      "book Rick human aleve choose heartburn choice crime eye Tanya\n",
      "Topic #2 (18, '0.008*\"Paige\" + 0.004*\"Mario\" + 0.004*\"Fiorina\" + 0.004*\"Carly\" + 0.004*\"explode\" + 0.003*\"Charleston\" + 0.003*\"assault\" + 0.003*\"hotel\" + 0.002*\"Evan\" + 0.002*\"10:30\"')\n",
      "Paige Mario Fiorina Carly explode Charleston assault hotel Evan 10:30\n",
      "Topic #3 (34, '0.015*\"jackpot\" + 0.014*\"powerball\" + 0.013*\"million\" + 0.013*\"400\" + 0.011*\"drawing\" + 0.009*\"$\" + 0.009*\"prize\" + 0.007*\"Wednesday\" + 0.007*\"lottery\" + 0.005*\"Jackpot\"')\n",
      "jackpot powerball million 400 drawing $ prize Wednesday lottery Jackpot\n",
      "Topic #4 (35, '0.012*\"Avery\" + 0.007*\"documentary\" + 0.007*\"Netflix\" + 0.007*\"avery\" + 0.006*\"Steven\" + 0.006*\"murder\" + 0.005*\"viewer\" + 0.005*\"petition\" + 0.005*\"pardon\" + 0.004*\"netflix\"')\n",
      "Avery documentary Netflix avery Steven murder viewer petition pardon netflix\n",
      "Topic #5 (53, '0.005*\"discreet\" + 0.004*\"underwear\" + 0.004*\"leak\" + 0.003*\"undo\" + 0.003*\"curve\" + 0.002*\"Stadium\" + 0.002*\"bladder\" + 0.002*\"coupon\" + 0.002*\"discreet.com\" + 0.002*\"Gillette\"')\n",
      "discreet underwear leak undo curve Stadium bladder coupon discreet.com Gillette\n",
      "['GM Moreland snapchat clothes track fleet Peyton gear compete develop', 'Subaru Diego tree Louis Raiders St. outback twenty-sixteen documentary Charles', 'prediction Lewinsky Taliban Verizon AT&amp;T governor 911 Monica Jake liberal', 'brewing raging siding efficient S.E.A.L. Ethan affluent drone Navy nightline', 'misdeed Jews burglary chapel complication Aden worship one-time freely subsubsidise', 'snore Banfield Ferrera elders End comedy Ashleigh grunt superstore halted', 'gun president executive Obama action background Congress check law control', 'aleve Brad tylenol knee PBS WAN pain na coaching six', 'Scott slager rose April Walter mouth golden salute creek n', 'Prince insulin helmet William starbucks Carpenter cashier sugar precious Toujeo', 'wreckage anticipation Crest va kate electrical attempt whitestrips barely reclaim', 'deductible earn hydroplane clockwork annually ™ fund limit apply savings', 'citracal pm lift Aleve Motors levy hopelessness chewable defiance darkness', 'march Glendora protestor 31-year-old foothill grievance Shiite psychological Colby truck', 'internet ha dough unleash hughesnet Jingle playstation gross downtime Sony', 'abreva belongings Hartford stelara ah virus heal 420 cookie Lo', 'say Saudi think people Iran can new want like now', 'snow County series local area winter 18 Ventura resident potential', 'Paige Mario Fiorina Carly explode Charleston assault hotel Evan 10:30', 'Iowa Cruz chantix caucus front-runner Bush administration candidate 2016 rival', 'milk aw Venezuela 12th lactose hackler bee SNL Assembly VICKSBURG', 'newswise Coricidin HBP projection ceo park arson wheat Debbie tweet', 'Erica react chapter 419 verbal Hill flurry Presidencia Israelis Jindal', 'freeway southbound rainfall sandbag Valley mud westbound head northbound 5', 'banquet hammonds grazing zero Rico acre Harney Puerto Cliven Botox', 'overseas Rossen Baghdad Herera Mathisen captioning corona Tyler consideration closed', '® bleeding cosby doctor blood Eliquis Cosby 24/7 Camille makeup', 'veteran fixodent trump-clinton uphold flawing european Ricky Gervais dignity :40', 'clot er dvt hannity pe hiker apparel bypass paige motel', 'NFL team chargers Angeles Los Rams owner application Oakland football', 'sexist stump HAMMONDS policy accusation poach infidelity basic Mitt Romney', 'Afghanistan seem debate girl okay News operation Carolina chief Sunday', 'volkswagen blooded bronco Pierre maam September Justice seed OSWEILER Adams', 'rain storm morning see area weather tomorrow look temperature week', 'jackpot powerball million 400 drawing $ prize Wednesday lottery Jackpot', 'Avery documentary Netflix avery Steven murder viewer petition pardon netflix', 'Subaru sedan I.T. legacy Guy 5th hotel midsize longest-lasting Kendall', 'shout collide yen bus rescuer MTA superstar six-story Indonesia Mission', 'electronics 401 Debbie k consumer Medina church native stow Mekahlo', 'victoza ® Victoza Seinfeld diabetes sugar Australia scout shave tighter', 'dog CPR bonus Diana firefighter si Compton showdown fervent schneider', 'doctor pain coach cough symptom medicine cold may relief asthma', 'point second world bring watch place number deal America end', 'prune delight amaz!n Jimmy coup delicious pepper Dean sunsweet fruit', 'cialis medical CIALIS erection daily romantic dysfunction nitrate erectile unsafe', 'vitamint centrum multivitamin 134 refreshingly mint cute clever smooth spectrum', 'stool Dulcolax tamiflu relief antiviral suppository softener dependable dulcolax constipate', 'float parade kitten purple dye abandon Pasadena MMMM display chew', 'AMEX 15/20 short-term hangover mideast pablo merry inventory 1/2 Bowen', 'Wars Star gas recede avatar price attic exterminator zumba squirrel', 'pastor church man Wright tape walk nope boat capsize veteran', 'Butler spotty Dubai dog Hallie VA marijuana militarization Gloria Tim', 'nae Clark Mara Nations bam Abe bust forest saudi-led Oprah', 'discreet underwear leak undo curve Stadium bladder coupon discreet.com Gillette', 'angrier recruiting Friends friends &amp; feature 58 tick Guard african', 'XARELTO xarelto Xarelto listerine bleed brake afib allstate ® quicker', 'tum heartburn smoothie tailgate nexium frequent protection neutralize tongue acid', 'stiffen bottom cbs2 street till prop highway steelers Cincinnati Washington', 'unquote legislative Speaker 370 tweet nbcla.com subvert rebound forest sentiment', 'tonight jewish finally starbuck realtime Hill Fareed Zakaria Daniella Capitol', 'suspect police arrest officer crime murder pursuit shoot shooting breathe', 'accent enroll IRS coveredca.com in-person 31st penalty good-bye visit Amerivest', 'Cosby testify deposition Camille assault cosby criminal defamation comedian Bill', 'emotional Rx grader highway recovery thoughtfully housekeeping caregiver repute Newtown', 'CNN m break holder news ticket Rezaian 424,000 7-ELEVEN Jeanne', 'pizza nicoderm cq closed delivery stab trip IRC Anna Parker', 'book Rick human aleve choose heartburn choice crime eye Tanya', '&lt; Warner Time Internet cable meg wine 300 39.99 Meg', 'China market stock walnut simple chinese economy healthy investor heart', 'crestor Peyton bronco manning Gop Pop 49ers render e-cigarette Tomsula', 'Granada hills fortunately knock firefighter flame puncture tlc SMURF tear', 'Dow trading Wall Street point asian 210 plunge Alisyn 350', 'bed sleep Gasparyan gadget lowest Artyom I8 hardcore preference divert', 'Clinton federal land Oregon Donald armed Trump Hillary campaign protester', 'newsroom Tokyo Lexus downgrading 8:00 p.m. Japan proxy spider es350', 'S.W.A.T. flavor shrimp grilled wake unimportant Detroit spicy pepper crock', 'harvonus mixup militants tookistock hep newsroom independently Juana cure relax', 'leak gas valve retirement Zicam shorten tracker snap Rowe duck', 'barricade bat Simi Williams baseball Raymond 57 nephew slope Valley', 'drone headline duel FAA draft briefing airport port plane frank', 'shingle dentures particle Blue chickenpox outbreak Poligrip drone dow listeria', 'Craig Strickland trading band hunting Dow duck singer manufacturing body', 'Humira infection Miss universe tb crown symptom Harvey Colombia Steve', 'Sanjay O&apos;Reilly Gupta tatum muse complex channing gaming heroine eg', 'fraud Mainstream Adeleson discount B brutality scorer R hat Review', 'oral-b freshpet brush sympathy Hillshire golden heroin flea Istanbul manual', 'couch Mexico probation teen extradition mother drunk Texas Affluenza deportation', 'Californians someplace icy bedroom robber rig boy Pentagon nearly biz', 'paper Boston subscriber delivery Globe globe deliver newspaper chipotle duty', 'Beth Taliban indian consulate o mm operator Kabul space dubt', 'Kelly volatile predict sexism volcano Amy Sheikh buyer menu insist', 'cam Manhattan 1.15 Newton Colorado credit Experian Lehman Brothers median', 'Mississippi water Brown Illinois Vegas Las River Missouri flood ship', 'theraflu expressmax auction Hover injury CNN.COM ageing fish t. tuna', 'Bernardino employee center San regional terror 14 shooting memorial Warner', 'nowadays revealing Iceland judge landery deplete veteran karin hygiene transport', 'cat bus coffee tidy latte macchiato drink espresso sight starbucks', 'dominant hungry Bo Motors humanity melon ice Iceland no-no Loesch', 'Ramadi iraqi Jane Garcetti Mayor Eric bomber suicide &amp; LIGHT', 'Marcy Sarah &gt; priest Christina 9:55 exit 9-year-old Jersey hoverboard']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, models\n",
    "from gensim.models import Phrases\n",
    "from sklearn import decomposition\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# initialize tokenizer and stopwords\n",
    "en_stop = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as',\n",
    "           'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot',\n",
    "           'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each',\n",
    "           'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\",\n",
    "           \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\",\n",
    "           \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more',\n",
    "           'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought',\n",
    "           'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should',\n",
    "           \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "           'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to',\n",
    "           'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\",\n",
    "           'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",\n",
    "           'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself',\n",
    "           'yourselves', 'apos', '&apos;m', '&apos;re', '&apos;s', 's', 'I', 'will', 'go', 'get', '(', ')', '?', ':', ';', ',', '.', '!',\n",
    "           '/', '\"', \"'\", \"...\",\"``\", \"&apos\", \"&apos;s\", \"&apos;&apos;\", \"-lsb-\", \"-rsb-\", \"-lcb-\", \"-rcb-\", \"-lrb-\", \"-rrb-\",\n",
    "           \"O&apos;MALLEY\", \"--\", \" \"]\n",
    "en_stop_stories = ['(', ')', '?', ':', ';', ',', '.', '!', '/', '\"', \"'\", \"...\",\"``\", \"-lsb-\", \"-rsb-\", \"-lcb-\", \"-rcb-\", \"-lrb-\", \"-rrb-\", \"--\", \" \"]\n",
    "\n",
    "stop_chars = ['<', '>']\n",
    "\n",
    "# get all lemmas between a <story>-</story>-pair:\n",
    "stories = []\n",
    "lemma_stories = []\n",
    "\n",
    "with open('3-4-5_story_test.txt') as infile:\n",
    "    for line in infile:\n",
    "        l = line.rstrip()\n",
    "        if l == \"<story>\":\n",
    "            story = []\n",
    "        elif l == \"</story>\":\n",
    "            stories.append(story)\n",
    "        elif not any(stop_char in l for stop_char in stop_chars):\n",
    "            if l not in en_stop:\n",
    "                story.append(l)\n",
    "\n",
    "\n",
    "with open('3-4-5_lemma_test.txt') as infile:\n",
    "    for line in infile:\n",
    "        l = line.rstrip()\n",
    "        if l == \"<story>\":\n",
    "            story = []\n",
    "        elif l == \"</story>\":\n",
    "            lemma_stories.append(story)\n",
    "        elif not any(stop_char in l for stop_char in stop_chars):\n",
    "            if l not in en_stop:\n",
    "                story.append(l)\n",
    "\n",
    "# create dictionary and wordcounts corpus:\n",
    "dictionary = corpora.Dictionary(lemma_stories)\n",
    "#print(dictionary.token2id)\n",
    "#dictionary.save(\"wordcounts.dict\")\n",
    "\n",
    "print(len(dictionary))\n",
    "\n",
    "\n",
    "# Bag-of-words representation of the stories.\n",
    "corpus = [dictionary.doc2bow(story) for story in lemma_stories]\n",
    "#print(corpus)\n",
    "#corpora.MmCorpus.serialize(\"corpus.mm\", corpus)\n",
    "print(len(corpus))\n",
    "print(len(stories))\n",
    "print(len(lemma_stories))\n",
    "\n",
    "# create tf.idf model:\n",
    "tfidf_model = models.TfidfModel(corpus)\n",
    "#tfidf_model.save(\"tfidf_model\")\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "#print(tfidf_corpus)\n",
    "#print(tfidf_corpus.chunksize)\n",
    "# corpora.MmCorpus.serialize(\"tfidf_corpus.mm\", tfidf_corpus)\n",
    "#\n",
    "# # create topic models:\n",
    "# LDA\n",
    "num_topics = 100\n",
    "\n",
    "lda_model = models.ldamodel.LdaModel(corpus=tfidf_corpus, id2word=dictionary, num_topics=num_topics, update_every=0, chunksize=5000, passes=20)\n",
    "#lda_model.save(\"lda_model\")\n",
    "lda_corpus = lda_model[tfidf_corpus]\n",
    "#corpora.MmCorpus.serialize(\"lda_corpus.mm\", lda_corpus)\n",
    "\n",
    "print(\"\\nTopics by Latent Dirichlet Allocation model\")\n",
    "topics_found_lda = lda_model.print_topics(num_topics=5, num_words=10)\n",
    "all_topics = lda_model.print_topics(num_topics=100, num_words=10)\n",
    "\n",
    "# print(topics_found_lda)\n",
    "\n",
    "\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for t in topics_found_lda:\n",
    "    print(\"Topic #{} {}\".format(counter, t))\n",
    "    words = re.findall('\"([^\"]+)\"', t[1])\n",
    "    words = ' '.join(words)\n",
    "    print(words)\n",
    "    counter += 1\n",
    "\n",
    "topics = []\n",
    "for t in all_topics:\n",
    "    words = re.findall('\"([^\"]+)\"', t[1])\n",
    "    words = ' '.join(words)\n",
    "    topics.append(words)\n",
    "\n",
    "print(topics)\n",
    "topics.extend((\"Stories with actual words\", \"Number of subelements in stories with actual words\", \"Stories with lemmas\",\n",
    "               \"Number of subelements in stories with lemmas\"))\n",
    "len(topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16, 0.25743585570059729), (65, 0.56170294158152745)]\n"
     ]
    }
   ],
   "source": [
    "lda_df = pd.DataFrame(columns=range(100))\n",
    "\n",
    "for i in range(len(lemma_stories)):\n",
    "    doc = lda_corpus[i]\n",
    "    for top, prob in doc:\n",
    "        lda_df.set_value(i, top, prob)\n",
    "print(doc)\n",
    "\n",
    "with open(\"3-4-5_Output.txt\", \"w\") as text_file:\n",
    "    text_file.write(\"Doc: %s\" % (doc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_df.to_csv(\"1-2-3_df.csv\")\n",
    "#lda_df\n",
    "# with open(\"1-2-3_lemma.txt\", \"w\") as lemma_file:\n",
    "#     for item in lemma_stories:\n",
    "#         lemma_file.write(\"%s\\n\" % item)\n",
    "# with open(\"1-2-3_story.txt\", \"w\") as story_file:\n",
    "#     for item in stories:\n",
    "#         story_file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3731\n",
      "3734\n",
      "3731\n"
     ]
    }
   ],
   "source": [
    "lemma_stories2 = [x for x in lemma_stories if x]\n",
    "stories2 = [x for x in stories if x]\n",
    "\n",
    "with open(\"3-4-5_new_lemma.txt\", \"w\") as lemma_file:\n",
    "    for item in lemma_stories2:\n",
    "        lemma_file.write(\"%s\\n\" % item)\n",
    "with open(\"3-4-5_new_story.txt\", \"w\") as story_file:\n",
    "    for item in stories2:\n",
    "        story_file.write(\"%s\\n\" % item)\n",
    "print(len(lemma_stories2))\n",
    "print(len(stories2))\n",
    "print(len(lda_df))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Extreme', 'weather', 'Cleanup', 'efforts', 'way', 'Illinois', 'Missouri', 'floodwater', 'pumped', 'overflowing', 'roadways', 'In', 'Alexander', 'County', 'Illinois', '100', 'homes', 'flooded', 'levees', 'strain', 'keep', 'water', 'spilling', 'Then', 'St.', 'Louis', 'flooding', 'concerns', 'real', 'swollen', 'Mississippi', 'River', 'least', '31', 'people', 'died', 'state', 'due', 'historic', 'flooding', 'Those', 'headlines', 'See', 'You', 'shortly', 'Monday', 'morning', 'Water', 'water', 'everywhere', 'Thank', 'much']\n"
     ]
    }
   ],
   "source": [
    "# print(\"stories:\")\n",
    "# for x in stories2:\n",
    "#     if set(x).issubset(en_stop)== True:\n",
    "#         print(x)\n",
    "#     elif len(x) < 3:\n",
    "#         print(x)\n",
    "\n",
    "#del stories2[1585]\n",
    "print(stories2[1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2961"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lda_df[-1] = stories2\n",
    "lda_df[-2] = lda_df[-1].apply(lambda x: len(x))\n",
    "lda_df[-3] = lemma_stories2 # Not a good indicator\n",
    "lda_df[-4] = lda_df[-3].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lda_df.loc[-1] = topics  # adding a row\n",
    "\n",
    "lda_df.index = lda_df.index + 1  # shifting index\n",
    "\n",
    "lda_df = lda_df.sort_index() # moving up\n",
    "\n",
    "lda_df.reset_index(drop=True, inplace=True)\n",
    "lda_df.to_csv('3-4-5_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
