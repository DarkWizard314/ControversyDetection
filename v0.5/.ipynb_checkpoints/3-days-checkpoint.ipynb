{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28918\n",
      "3174\n",
      "3174\n",
      "3174\n",
      "<gensim.interfaces.TransformedCorpus object at 0x12a6d5f28>\n",
      "None\n",
      "\n",
      "Topics by Latent Dirichlet Allocation model\n",
      "Topic #1 (41, '0.006*\"captioning\" + 0.005*\"extra\" + 0.005*\"consideration\" + 0.005*\"sheriff\" + 0.004*\"closed\" + 0.004*\"Olay\" + 0.003*\"provide\" + 0.002*\"Johnny\" + 0.002*\"regenerist\" + 0.002*\"ndromat\"')\n",
      "captioning extra consideration sheriff closed Olay provide Johnny regenerist ndromat\n",
      "Topic #2 (88, '0.004*\"tattoo\" + 0.004*\"Japan\" + 0.003*\"doll\" + 0.003*\"photograph\" + 0.003*\"girl\" + 0.002*\"10-year-old\" + 0.002*\"beauty\" + 0.002*\"Brazil\" + 0.002*\"orchestra\" + 0.002*\"breakthrough\"')\n",
      "tattoo Japan doll photograph girl 10-year-old beauty Brazil orchestra breakthrough\n",
      "Topic #3 (86, '0.015*\"Rogers\" + 0.010*\"Wayne\" + 0.008*\"complication\" + 0.007*\"pneumonia\" + 0.007*\"trapper\" + 0.005*\"die\" + 0.004*\"mash\" + 0.004*\"John\" + 0.004*\"investor\" + 0.003*\"estate\"')\n",
      "Rogers Wayne complication pneumonia trapper die mash John investor estate\n",
      "Topic #4 (73, '0.003*\"picture\" + 0.002*\"gallery\" + 0.002*\"infant\" + 0.002*\"snow\" + 0.002*\"aaa\" + 0.002*\"shovel\" + 0.002*\"94\" + 0.002*\"encouraging\" + 0.002*\"Souza\" + 0.002*\"115\"')\n",
      "picture gallery infant snow aaa shovel 94 encouraging Souza 115\n",
      "Topic #5 (64, '0.010*\"24/7\" + 0.008*\"align\" + 0.008*\"digestive\" + 0.005*\"probiotic\" + 0.005*\"undisputed\" + 0.005*\"GE\" + 0.004*\"kind-of-day\" + 0.004*\"hold-onto-your-tiara\" + 0.004*\"sweet-treat-goodness\" + 0.003*\"#\"')\n",
      "24/7 align digestive probiotic undisputed GE kind-of-day hold-onto-your-tiara sweet-treat-goodness #\n",
      "['Roma saudi Harriet Tamron worker supervisor madness scour GMA 24-year-old', 'boost vitamin blooded ballot Alisyn signature nutritional mediabuzz essential Kennel', 'Brian 25-year-old Jennifer stab Fitch football icon suv effective Kevin', 'spectacles comeback backup 31 trophy lte 5,500 triple oregon Rolwland', 'aleve tylenol sinus knee Brad pain Alka-Seltzer meditation WAN Aveeno', 'Jenner CAITLYN lawsuit resolve crash car fatal legal cause connect', 'sleep bed 600 lowest insider hardcore I8 enter Odom amnesty', 'credit Akron rancher dog hole Sierra Madre traditional convict mayor', 'Saudi Arabia Iran cleric embassy execute protest Nimr shiite Tehran', 'wine globe pretend peanut equal Francis noir Pinot good-bye provider', 'soar retailer Gourdon EUBANKS cereal boycott Edwards Stephanie shelf broadcast', 'shark Kooiman Anna custer gesture 1st inferno globe Anwar eyewitness', 'inferno luxury skyscraper Dubai discovery hotel tower firework Klemack indian', 'hughesnet Hughesnet internet firefighter dog Cross Red gen4 satellite stray', 'spill dentures particle carjacking contract phone Poligrip installment Burbank petroleum', 'calorie bacterium denture nutrition fruit brunch Oz clean Jason Daphne', 'baby eat real without carry become health care effect young', 'victim crash car shoot CBS freeway Ontario student trial I.S.I.L.', 'year new people say see can like come one know', 'deductible insurance Mutual Liberty pay $ earn 100 fund ditch', 'aleve pain pill seriously rob deliveries ciabatta arthritis credit Jill', 'Sobel ethan pair Honda misdeed Pope elevator confines Adele golden', 'season Pop times paper mental scaffolding neither stuntman stunt kousent', 'Verizon Michelle m-16 Lea AT&amp;T Gabrielle Mercedes priest enabler omelet', 'sig northbound 101 striking crosshair Columbus zoo youngest downtown eight-week-old', 'Fiorina Carly season Los Angeles Eastern gas communication picture neighborhood', '&lt; Brown publicist drug Las Vegas Rowland 10:30 skip assault', 'Clemson game zero Alabama bowl championship quarterback touchdown xr Oklahoma', 'case Cosby news float continue NBC still drive moment $', 'Cole Natalie discretion mature unforgettable advise viewer subject due matter', 'prove statute spanish employee limitation charge Montgomery ipad Radcliffe next-door', 'oral-b reopen brush manual clean terror closed center developmental IRC', 'nicoderm cq outbreak Park governor patch smoke airport extended listeria', 'prosecution Fred gray cable trial tribunal pope genocide shelter Pope', 'Blackwell Victor Kosik Alison Christi main heights salt castle rupture', 'India Rousey Ronda lag Lydia faction marine tap revolution leadership', 'blaze concussion Manziel skyscraper Leslie belongings renter injure nearby season', 'king duel draft baby wrinkle bloodworth Sir chaotic Amanda Bloodworth', 'La. silence Jazeera.com Lui brainiac Camby consulate 630 RESEDA hypothermia', 'CNN break news Kardashian meta Stone fund-raising Burj block Khalifa', 'volatility soma Adrian gambling gurule 6:45 debt dm juggle accusation', 'captioning extra consideration sheriff closed Olay provide Johnny regenerist ndromat', 'one-month stow offline Sprint Jackson gorilla goofy councilman band insider', 'Meade Robin Smith Lynn anncr Barbie Lynne milk fish Keiko', 'Eliquis bleeding clot ELIQUIS dvt doctor eliqui eliquis pe 15/20', 'cargill thief tan Anderson Camarillo Cooper Rio outlet infiltrate garth', 'Subaru earthquake jinkster 4.2 farmer 250 5:30 bleep sixty-five Oklahoma', 'epic Expedia starbucks throat mechanical familis 63-year-old stall boxing superbug', 'Lexus lease cigarette dealer callender 349 es350 sale pie gs350', 'nexium staples frequent HBP Coricidin heartburn xi fraction Leo activia', 'troops volunteer Nespresso battlefield Environmental Medicine stomach toboggan Institute Syrians', 'Aviv Tel gunman couch Israel Ethan israeli Mexico restaurant kill', 'Jane catch USAA worldwide LIGHT friends middle - Jon Fox', 'insulin sugar Â® Toujeo toujeo diabetes ole Seth long-acting Associated', 'El fun Nino girl shower sunshine advisory pattern embrace warmer', 'Pirro bruin Gillette lyft Stadium 18,000 metrohealth port skate Legal', 'Jeffrey o&apos;reilly aggressively nuggets Blackhawk purposely disturbed prisoner cgi Valentine', 'water flood Mississippi Missouri river River inmate crest levee Louis', 'moose 382 hymn stone Brooks binge honking horn Wong Garth', 'Miguel brace Almaguer nowadays cripple Falls hall region turbine quiet', 'NBC News Pop press Chuck Snow Todd tougher Kate pop', 'Laquan McDonald Chicago mayor e-mail Shiite Indra Japan Rosa Abe', 'cricket harsh extra insistent hopelessness poverty abused heartache Adelson ambition', 'rope pool exterior 15,000 EXPEDIA mom celestial fifteen crack squirrel', '24/7 align digestive probiotic undisputed GE kind-of-day hold-onto-your-tiara sweet-treat-goodness #', 'credit Curry llama warrior Legend karma Steph score bid 11:00', 'parade traffic breach rise fantasy Holly tournament diversity Pasadena Illinois', 'chantix informant machete quit Chantix ski Walmart mood mask knife', 'shakeup al-shabab Abreva chairman Guantanamo crab spider Hummer hummer oncoming', 'secretary billionaire stranger wages tease steering sob mechanic beluga overhear', 'execution protester saudi Saudis angry condemn iranian protest Riyadh Arabian', 'heartburn reliefchews Alka-Seltzer Bundy lottery Hammond bachelor packaging Ron chalky', 'vitamint Biotene Chevy centrum dry multivitamin volatile birth smooth biotene', 'picture gallery infant snow aaa shovel 94 encouraging Souza 115', 'hungry overshopping equal 5500 8,000 Daniella newsroom Divine Dominican draftking', 'Chris Las Brown Vegas Christie humpback singer whale Jersey governor', 'whale tank seaworld breed personality ted frank wilderness SEAWORLD futurecast', 'Hover millennial inhalation brilliance smile 3d Board suburban regular stain', 'raisin Zicam Jarvis uphold squawk muse Riverside uncontrollably shorten Canyon', 'Travis Kenia switch cable Time Warner Forbes Denver card one-time', 'tum York Square Times injury FBI Gray e-mail City defense', 'campaign Carson every charge candidate U.S. change fight Ben report', '54-year-old Heights wrestle 33-year-old deputy 27-year-old Roland gunshot undo achievable', 'shave exciting lineup hydrophobic fallout goal Palmdale beautiful thermometer scarf', 'heartburn OTC isis-inspired Hartman Cuba hatch ahhh PRILOSEC prilosec zero', 'Humira infection symptom tb remission crohn occupy arson Bank member', 'Rogers Wayne complication pneumonia trapper die mash John investor estate', 'aski Oz flavor tour uptown education pastor Bruno preschool surprising', 'tattoo Japan doll photograph girl 10-year-old beauty Brazil orchestra breakthrough', '63-story wage minimum ballistic missile sergeant Schaefer sanction biking stepson', 'iraqi Ramadi suicide station attack train bomber Munich Germany city', 'wind degree Angeles Los weather a.m. Rose track Bowl temperature', 'Oregon refuge wildlife protester land rat rhetoric Shabaab pizza shout', 'desk sport Weekend Express Buffalo McDANIEL walking Georgia 405 southbound', 'insurance rate guarantee Mutual accident Liberty lock perfect information free', 'Lakers Paraguay flash Uber 200 Fed overtop ancestry growth splash', 'voter outsider teen so-called Affluenza dramatic bond wrong Benghazi mom', 'vacation page lifelock VA Inthe Wright homegrown lion read flag', 'conquer ebola rocket all-weather adaptability tow capability Lexus perfection receipt', 'rain morning snow storm temperature forecast Dubai Monday tomorrow dry']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, models\n",
    "from gensim.models import Phrases\n",
    "from sklearn import decomposition\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# initialize tokenizer and stopwords\n",
    "en_stop = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as',\n",
    "           'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot',\n",
    "           'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each',\n",
    "           'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\",\n",
    "           \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\",\n",
    "           \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more',\n",
    "           'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought',\n",
    "           'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should',\n",
    "           \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "           'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to',\n",
    "           'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\",\n",
    "           'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",\n",
    "           'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself',\n",
    "           'yourselves', 'apos', '&apos;m', '&apos;re', '&apos;s', 's', 'I', 'will', 'go', 'get', '(', ')', '?', ':', ';', ',', '.', '!',\n",
    "           '/', '\"', \"'\", \"...\",\"``\", \"&apos\", \"&apos;s\", \"&apos;&apos;\", \"-lsb-\", \"-rsb-\", \"-lcb-\", \"-rcb-\", \"-lrb-\", \"-rrb-\",\n",
    "           \"O&apos;MALLEY\", \"--\", \" \"]\n",
    "en_stop_stories = ['(', ')', '?', ':', ';', ',', '.', '!', '/', '\"', \"'\", \"...\",\"``\", \"-lsb-\", \"-rsb-\", \"-lcb-\", \"-rcb-\", \"-lrb-\", \"-rrb-\", \"--\", \" \"]\n",
    "\n",
    "stop_chars = ['<', '>']\n",
    "\n",
    "# get all lemmas between a <story>-</story>-pair:\n",
    "stories = []\n",
    "lemma_stories = []\n",
    "\n",
    "with open('1-2-3_story_test.txt') as infile:\n",
    "    for line in infile:\n",
    "        l = line.rstrip()\n",
    "        if l == \"<story>\":\n",
    "            story = []\n",
    "        elif l == \"</story>\":\n",
    "            stories.append(story)\n",
    "        elif not any(stop_char in l for stop_char in stop_chars):\n",
    "            if l not in en_stop_stories:\n",
    "                story.append(l)\n",
    "\n",
    "\n",
    "with open('1-2-3_lemma_test.txt') as infile:\n",
    "    for line in infile:\n",
    "        l = line.rstrip()\n",
    "        if l == \"<story>\":\n",
    "            story = []\n",
    "        elif l == \"</story>\":\n",
    "            lemma_stories.append(story)\n",
    "        elif not any(stop_char in l for stop_char in stop_chars):\n",
    "            if l not in en_stop:\n",
    "                story.append(l)\n",
    "\n",
    "# create dictionary and wordcounts corpus:\n",
    "dictionary = corpora.Dictionary(lemma_stories)\n",
    "#print(dictionary.token2id)\n",
    "#dictionary.save(\"wordcounts.dict\")\n",
    "\n",
    "print(len(dictionary))\n",
    "\n",
    "\n",
    "# Bag-of-words representation of the stories.\n",
    "corpus = [dictionary.doc2bow(story) for story in lemma_stories]\n",
    "#print(corpus)\n",
    "#corpora.MmCorpus.serialize(\"corpus.mm\", corpus)\n",
    "print(len(corpus))\n",
    "print(len(stories))\n",
    "print(len(lemma_stories))\n",
    "\n",
    "# create tf.idf model:\n",
    "tfidf_model = models.TfidfModel(corpus)\n",
    "#tfidf_model.save(\"tfidf_model\")\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "print(tfidf_corpus)\n",
    "print(tfidf_corpus.chunksize)\n",
    "# corpora.MmCorpus.serialize(\"tfidf_corpus.mm\", tfidf_corpus)\n",
    "#\n",
    "# # create topic models:\n",
    "# LDA\n",
    "num_topics = 100\n",
    "\n",
    "lda_model = models.ldamodel.LdaModel(corpus=tfidf_corpus, id2word=dictionary, num_topics=num_topics, update_every=0, chunksize=5000, passes=20)\n",
    "#lda_model.save(\"lda_model\")\n",
    "lda_corpus = lda_model[tfidf_corpus]\n",
    "#corpora.MmCorpus.serialize(\"lda_corpus.mm\", lda_corpus)\n",
    "\n",
    "print(\"\\nTopics by Latent Dirichlet Allocation model\")\n",
    "topics_found_lda = lda_model.print_topics(num_topics=5, num_words=10)\n",
    "all_topics = lda_model.print_topics(num_topics=100, num_words=10)\n",
    "\n",
    "# print(topics_found_lda)\n",
    "\n",
    "\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for t in topics_found_lda:\n",
    "    print(\"Topic #{} {}\".format(counter, t))\n",
    "    words = re.findall('\"([^\"]+)\"', t[1])\n",
    "    words = ' '.join(words)\n",
    "    print(words)\n",
    "    counter += 1\n",
    "\n",
    "topics = []\n",
    "for t in all_topics:\n",
    "    words = re.findall('\"([^\"]+)\"', t[1])\n",
    "    words = ' '.join(words)\n",
    "    topics.append(words)\n",
    "\n",
    "print(topics)\n",
    "topics.extend((\"Stories with actual words\", \"Number of subelements in stories with actual words\", \"Stories with lemmas\",\n",
    "               \"Number of subelements in stories with lemmas\"))\n",
    "len(topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12, 0.025294073823271451), (16, 0.075836257012493163), (18, 0.17140214755576191), (28, 0.13294492818859602), (54, 0.031292357755426124), (81, 0.092850659049904818), (88, 0.30435520817457506), (91, 0.065944171474252647)]\n"
     ]
    }
   ],
   "source": [
    "lda_df = pd.DataFrame(columns=range(100))\n",
    "\n",
    "for i in range(len(lemma_stories)):\n",
    "    doc = lda_corpus[i]\n",
    "    for top, prob in doc:\n",
    "        lda_df.set_value(i, top, prob)\n",
    "print(doc)\n",
    "\n",
    "with open(\"1-2-3_Output.txt\", \"w\") as text_file:\n",
    "    text_file.write(\"Doc: %s\" % (doc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_df.to_csv(\"1-2-3_df.csv\")\n",
    "#lda_df\n",
    "with open(\"1-2-3_story.txt\", \"w\") as story_file:\n",
    "    for item in stories:\n",
    "        story_file.write(\"%s\\n\" % item)\n",
    "with open(\"1-2-3_lemma.txt\", \"w\") as lemma_file:\n",
    "    for item in lemma_stories:\n",
    "        lemma_file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3097\n",
      "3097\n",
      "3097\n"
     ]
    }
   ],
   "source": [
    "lemma_stories2 = [x for x in lemma_stories if x != []]\n",
    "stories2 = [x for x in stories if x]\n",
    "\n",
    "with open(\"1-2-3_new_lemma.txt\", \"w\") as lemma_file:\n",
    "    for item in lemma_stories2:\n",
    "        lemma_file.write(\"%s\\n\" % item)\n",
    "with open(\"1-2-3_new_story.txt\", \"w\") as story_file:\n",
    "    for item in stories2:\n",
    "        story_file.write(\"%s\\n\" % item)\n",
    "print(len(lemma_stories2))\n",
    "print(len(stories2))\n",
    "print(len(lda_df))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_df[-1] = lemma_stories2\n",
    "#lda_df[-2] = lda_df[-1].apply(lambda x: len(x))\n",
    "lda_df[-3] = lemma_stories2 # Not a good indicator\n",
    "lda_df[-4] = lda_df[-3].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lda_df.loc[-1] = topics  # adding a row\n",
    "\n",
    "lda_df.index = lda_df.index + 1  # shifting index\n",
    "\n",
    "lda_df = lda_df.sort_index() # moving up\n",
    "\n",
    "lda_df.reset_index(drop=True, inplace=True)\n",
    "lda_df.to_csv('1-2-3_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
