{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28918\n",
      "3174\n",
      "3174\n",
      "3174\n",
      "<gensim.interfaces.TransformedCorpus object at 0x11b7a7fd0>\n",
      "None\n",
      "\n",
      "Topics by Latent Dirichlet Allocation model\n",
      "Topic #1 (11, '0.003*\"newsroom\" + 0.003*\"PSH\" + 0.002*\"Dix\" + 0.002*\"margin\" + 0.002*\"clergyman\" + 0.001*\"rage\" + 0.001*\"justice\" + 0.001*\"malnutrition\" + 0.001*\"Ft.\" + 0.001*\"wanders\"')\n",
      "newsroom PSH Dix margin clergyman rage justice malnutrition Ft. wanders\n",
      "Topic #2 (32, '0.006*\"pastor\" + 0.004*\"Klemack\" + 0.003*\"theraflu\" + 0.003*\"expressmax\" + 0.002*\"Rachel\" + 0.002*\"Cadiz\" + 0.002*\"inferno\" + 0.002*\"aski\" + 0.002*\"lion\" + 0.002*\"Elton\"')\n",
      "pastor Klemack theraflu expressmax Rachel Cadiz inferno aski lion Elton\n",
      "Topic #3 (35, '0.007*\"equal\" + 0.006*\"inferno\" + 0.006*\"stock\" + 0.005*\"bank\" + 0.005*\"wine\" + 0.005*\"growth\" + 0.004*\"ally\" + 0.004*\"8,000\" + 0.003*\"release\" + 0.003*\"S&amp;P\"')\n",
      "equal inferno stock bank wine growth ally 8,000 release S&amp;P\n",
      "Topic #4 (50, '0.006*\"Blue\" + 0.004*\"listeria\" + 0.004*\"Bell\" + 0.004*\"outbreak\" + 0.004*\"pope\" + 0.003*\"broken\" + 0.003*\"deadly\" + 0.003*\"cream\" + 0.002*\"ice\" + 0.002*\"Carolinas\"')\n",
      "Blue listeria Bell outbreak pope broken deadly cream ice Carolinas\n",
      "Topic #5 (26, '0.003*\"Jeffrey\" + 0.002*\"Wallace\" + 0.002*\"stuntman\" + 0.002*\"Aleppo\" + 0.002*\"daydream\" + 0.002*\"countryside\" + 0.002*\"Hillcrest\" + 0.002*\"impose\" + 0.002*\"Aretha\" + 0.002*\"stunt\"')\n",
      "Jeffrey Wallace stuntman Aleppo daydream countryside Hillcrest impose Aretha stunt\n",
      "['militant one-month Shanna Baghdad tower priest Sprint Mendiola Hershey follower', 'Whitfield Fredricka futurecast Michaela revote duel coal Becky McCaffrey ramification', 'Tennessee 382 anncr milk peach us VICKSBURG surge nap Herzog', 'city help news head state move mean $ month back', 'frenzy Weekend Express Garcetti poach Polpot Nolan Conan overdrive subsubsidise', 'balloon wine Granada Simmons ancestry fishing Barbie bust xi azaz', 'Tamir Rice jury captioning protester consideration closed door prosecutor indict', 'Lexus es350 349 911 Lolita Victorville Chen lease blooded Lopez', 'ghost rowling JK buster zion amidst souther expansion cheapest Meredith', 'truecar sanction missile shed Iran pricing ballistic datum rope harsh', 'fun virtue haphazard dough boring unleash someone else smell share', 'newsroom PSH Dix margin clergyman rage justice malnutrition Ft. wanders', 'dentures particle concussion farmer chinese Poligrip Laden seal Manziel Johnny', 'Disney Wars Star Lucas lyft Mara Arkansas George sell freeway', 'Brown Las Vegas Chris singer publicist brown investigation woman resort', 'Oregon liquid TCU land refuge protester gel federal Seltzer Alka', 'couch Ethan Clemson Mexico teen Affluenza Alabama probation championship extradition', 'discretion mature advise viewer subject cq nicoderm due matter nowhere', 'boy true hug song build dance wall fashion Michael score', 'Verizon jinkster dough unleash chalkboard pop nightly goal derby demolition', 'Schlesinger outdoors Jill IBM windshield hospitalise goal Wharton consumer PRK', 'pain deductible aleve insurance 100 Mutual pay Liberty pill $', 'banquet fixodent natural earthquake tooth potato chicken 4.2 breast Oklahoma', 'wish offend front-runner weird dissident horrifying one-time awareness max mad', 'year new say people just come think like can know', 'rain storm forecast wind dry Monday weather snow guarantee cloudy', 'Jeffrey Wallace stuntman Aleppo daydream countryside Hillcrest impose Aretha stunt', 'Anderson Herera Cooper nightly Tyler Mathisen sue Hartman warfarin underwear', 'raid Mario Lydia cement bleach star hallway Quentin fame tallest', 'accident Rogers insurance Mutual Liberty mutual perfect liberty Wayne pneumonia', 'station plot train Germany bleeding Eliquis Munich celebrate terror clot', 'Meyer Ohio Notre Dame urban Carpenter coy coach wire mainland', 'pastor Klemack theraflu expressmax Rachel Cadiz inferno aski lion Elton', 'throne chapter universal fan read mend distract dubt longer unquestionably', 'insulin Tehran shia Â® protester sugar Toujeo toujeo Yemen diabetes', 'equal inferno stock bank wine growth ally 8,000 release S&amp;P', 'nuggets tuition production sh Kenya Lem NSHG Elvis u.v. vecede', 'turbine frank sitcom personality mourn tidal Taiwan FOLSOM familiar kpneu', 'Pop pop shutdown hungry enabler J. receipt lame insulting disapprove', '97 GEICO aim reduce discuss friendly millennial ndromat general caption', 'I.S.I.L. handgun Sousa Vallarta arrest army evade Reuters times Idaho', 'jackpot powerball Robin Smith Jazeera 334 Meade ticket lottery testimony', 'Jude vacation St. CAITLYN ESPN superhero Jenner foreman resolve lawsuit', 'period amnesty Ross counterintelligence mistrust 7:07 siege Nations Yemen Teheran', 'gun president executive Obama action Congress control violence background kitten', 'heartburn reliefchews Alka-Seltzer callender Marie pie MMM chalky keeper walnut', 'Saudi Arabia execution Iran cleric embassy execute Nimr shiite saudi', 'credit aleve tylenol score Rousey karma knee Brad rocket Toyota', 'tum smoothie tongue acid neutralize dissolve stomach instant heartburn War', 'Strickland anthem skipper 6:45 bejesus tick Alamo Embassey prize Laurel', 'Blue listeria Bell outbreak pope broken deadly cream ice Carolinas', 'Square Times tight oral-b reveler pack Bundy spectacular brush jam', 'WELC u-turn lineup volatility Aveeno miss. Kokomo soma debt slayer', 'collide freeway six-year-old grim Ontario blur explode boy direction scene', 'Rochester FBI machete informant mask knife arrest Walmart ski restaurant', 'stool Dulcolax Bazang Brookside suppository softener calorie dependable supervisor easesy', 'lifelock prove ultra FIFA monitor cascade USAA Charmin spanish trump-clinton', 'Japan Abe meditation Hot inflation Dominican Valerie hoe bellow Republic', 'Jenner twist installment carrier available lawsuit contract turmoil Olympic AT&amp;T', 'Subaru donation sixty-five donate 250 share love total vaccine Hart', 'fantasy trapper zero Kate winner Alice stock king select cotton', 'shave Abreva Minnesota architecture heal deer eighteen debut ground-breaking lion', 'weight caucus Jeb loss GOP voter career r&amp;b Trotter workout', 'correction cricket k Gilmore Boykin aderibigbe offender inmate rope Frankenstein', 'cosby accuser hike comedian son scene vow Heights Roland wrestle', 'heartburn OTC lockup thief tip Camarillo PRILOSEC ahhh prilosec false', 'chantix smoking quit doctor mood Chantix seizure stop suicidal spill', 'Palmdale pole zip re-air efficient driver appointment lash speeding Osteen', 'Gray dad wish Baltimore completely mayor red without e-mail honor', 'feud Gray Freddie Akron recall fraction drought Kelly mayor trump/clinton', 'Travis Forbes wherever inhalation detective gurule lane Eureka Tanya northbound', 'Kardashian ebola Seinfeld Nigeria Jerry 92 coffee marathon clockwork premium', 'water flood Aviv Tel gunman Mississippi river River Missouri Illinois', 'good-bye regularly spider toboggan scour justified web employment attraction verizon', 'cough rob neck hover medal smolder board carpenter 6:30 honor', 'chaos shake-up Armstrong Dr. blackberry Williams effective birthday goal 20,000', 'internet hughesnet Hughesnet satellite calorie mute Jason gen4 Americas Mercedes', 'campaign Clinton Trump Iowa Donald Carson president candidate Hillary trump', 'CNN break news cosby 10:30 Press Associated llama tweet silence', 'trump Muslims democratic recruitment video primary Donald Rubio Clinton recruit', 'Fleming earth Lord Inthe misdeed Globe Erica wardrobe crosswalk spring', 'bond Affluenza so-called teen Mars Andrea explain attorney spectacles Montgomery', 'Cosby resolution assault baby extra charge criminal girl carry plus', '&lt; Cross baby achievable Red mediabuzz Pomona salute drizzle post', 'Cole Natalie unforgettable cable singer Warner 65 switch grammy Time', 'periscope warmest editor book poppy maniac Harlow meerkat Janice 12-year-old', 'nexium cave rewrite battleship heartburn Iranians claustrophobic advertise burial demonstrator', 'annoying spreading throat Viagra alone beluga cross Riverside Erie opener', 'challenger voter BBC shore Jamaicans spotty trend Newman export jet', 'Pakistan productive book neighbour Pirro ode quash society modern DURST', 'levy fund-raising ya Post sleep mat melon Bordeaux Haiti upset', '24/7 align digestive mouth flu tamiflu # undisputed probiotic GE', 'homeless breach breaking Midwest 3 Channel 22 nightly overnight tired', 'tallest exterior stray reservation Roy list casing traveler parking Russell', 'bacterium denture brighter credit formula 99.99 POLIDENT fresher odor polident', 'kidney Stone Butler c James familiar possession sweeper kiosk duster', 'xr add afraid alzheimer vitamint India without inspiration worker 25', 'occupy massiv Bank exciting rare binge beautiful arson addiction typhoon', 'perfection conquer pursuit Lexus y all-weather adaptability Nissan dash Marin', 'lookout regain stranger sob ab innocent steering cop thousand Republic']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, models\n",
    "from gensim.models import Phrases\n",
    "from sklearn import decomposition\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# initialize tokenizer and stopwords\n",
    "en_stop = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as',\n",
    "           'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot',\n",
    "           'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each',\n",
    "           'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\",\n",
    "           \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\",\n",
    "           \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more',\n",
    "           'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought',\n",
    "           'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should',\n",
    "           \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "           'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to',\n",
    "           'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\",\n",
    "           'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",\n",
    "           'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself',\n",
    "           'yourselves', 'apos', '&apos;m', '&apos;re', '&apos;s', 's', 'I', 'will', 'go', 'get', '(', ')', '?', ':', ';', ',', '.', '!',\n",
    "           '/', '\"', \"'\", \"...\",\"``\", \"&apos\", \"&apos;s\", \"&apos;&apos;\", \"-lsb-\", \"-rsb-\", \"-lcb-\", \"-rcb-\", \"-lrb-\", \"-rrb-\",\n",
    "           \"O&apos;MALLEY\", \"--\", \" \"]\n",
    "en_stop_stories = ['(', ')', '?', ':', ';', ',', '.', '!', '/', '\"', \"'\", \"...\",\"``\", \"-lsb-\", \"-rsb-\", \"-lcb-\", \"-rcb-\", \"-lrb-\", \"-rrb-\", \"--\", \" \"]\n",
    "\n",
    "stop_chars = ['<', '>']\n",
    "\n",
    "# get all lemmas between a <story>-</story>-pair:\n",
    "stories = []\n",
    "lemma_stories = []\n",
    "\n",
    "with open('1-2-3_story_test.txt') as infile:\n",
    "    for line in infile:\n",
    "        l = line.rstrip()\n",
    "        if l == \"<story>\":\n",
    "            story = []\n",
    "        elif l == \"</story>\":\n",
    "            stories.append(story)\n",
    "        elif not any(stop_char in l for stop_char in stop_chars):\n",
    "                story.append(l)\n",
    "\n",
    "\n",
    "with open('1-2-3_lemma_test.txt') as infile:\n",
    "    for line in infile:\n",
    "        l = line.rstrip()\n",
    "        if l == \"<story>\":\n",
    "            story = []\n",
    "        elif l == \"</story>\":\n",
    "            lemma_stories.append(story)\n",
    "        elif not any(stop_char in l for stop_char in stop_chars):\n",
    "            if l not in en_stop:\n",
    "                story.append(l)\n",
    "\n",
    "# create dictionary and wordcounts corpus:\n",
    "dictionary = corpora.Dictionary(lemma_stories)\n",
    "#print(dictionary.token2id)\n",
    "#dictionary.save(\"wordcounts.dict\")\n",
    "\n",
    "print(len(dictionary))\n",
    "\n",
    "\n",
    "# Bag-of-words representation of the stories.\n",
    "corpus = [dictionary.doc2bow(story) for story in lemma_stories]\n",
    "#print(corpus)\n",
    "#corpora.MmCorpus.serialize(\"corpus.mm\", corpus)\n",
    "print(len(corpus))\n",
    "print(len(stories))\n",
    "print(len(lemma_stories))\n",
    "\n",
    "# create tf.idf model:\n",
    "tfidf_model = models.TfidfModel(corpus)\n",
    "#tfidf_model.save(\"tfidf_model\")\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "print(tfidf_corpus)\n",
    "print(tfidf_corpus.chunksize)\n",
    "# corpora.MmCorpus.serialize(\"tfidf_corpus.mm\", tfidf_corpus)\n",
    "#\n",
    "# # create topic models:\n",
    "# LDA\n",
    "num_topics = 100\n",
    "\n",
    "lda_model = models.ldamodel.LdaModel(corpus=tfidf_corpus, id2word=dictionary, num_topics=num_topics, update_every=0, chunksize=5000, passes=20)\n",
    "#lda_model.save(\"lda_model\")\n",
    "lda_corpus = lda_model[tfidf_corpus]\n",
    "#corpora.MmCorpus.serialize(\"lda_corpus.mm\", lda_corpus)\n",
    "\n",
    "print(\"\\nTopics by Latent Dirichlet Allocation model\")\n",
    "topics_found_lda = lda_model.print_topics(num_topics=5, num_words=10)\n",
    "all_topics = lda_model.print_topics(num_topics=100, num_words=10)\n",
    "\n",
    "# print(topics_found_lda)\n",
    "\n",
    "\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for t in topics_found_lda:\n",
    "    print(\"Topic #{} {}\".format(counter, t))\n",
    "    words = re.findall('\"([^\"]+)\"', t[1])\n",
    "    words = ' '.join(words)\n",
    "    print(words)\n",
    "    counter += 1\n",
    "\n",
    "topics = []\n",
    "for t in all_topics:\n",
    "    words = re.findall('\"([^\"]+)\"', t[1])\n",
    "    words = ' '.join(words)\n",
    "    topics.append(words)\n",
    "\n",
    "print(topics)\n",
    "topics.extend((\"Stories with actual words\", \"Number of subelements in stories with actual words\", \"Stories with lemmas\",\n",
    "               \"Number of subelements in stories with lemmas\"))\n",
    "len(topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0.15140280788508031), (18, 0.02110060898899134), (24, 0.2142175298386137), (29, 0.46049301790890557), (82, 0.043935332843486705), (92, 0.012203885130569782)]\n"
     ]
    }
   ],
   "source": [
    "lda_df = pd.DataFrame(columns=range(100))\n",
    "\n",
    "for i in range(len(lemma_stories)):\n",
    "    doc = lda_corpus[i]\n",
    "    for top, prob in doc:\n",
    "        lda_df.set_value(i, top, prob)\n",
    "print(doc)\n",
    "\n",
    "with open(\"1-2-3_Output.txt\", \"w\") as text_file:\n",
    "    text_file.write(\"Doc: %s\" % (doc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_df.to_csv(\"1-2-3_df.csv\")\n",
    "#lda_df\n",
    "with open(\"1-2-3_story.txt\", \"w\") as story_file:\n",
    "    for item in stories:\n",
    "        story_file.write(\"%s\\n\" % item)\n",
    "with open(\"1-2-3_lemma.txt\", \"w\") as lemma_file:\n",
    "    for item in lemma_stories:\n",
    "        lemma_file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-48-70456e868288>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-48-70456e868288>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    with open(\"1-2-3_new_lemma.txt\", \"w\") as lemma_file:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "lemma_stories2 = [x for x in lemma_stories if x != []]\n",
    "stories2 = [x for x in stories if x\n",
    "\n",
    "with open(\"1-2-3_new_lemma.txt\", \"w\") as lemma_file:\n",
    "    for item in lemma_stories2:\n",
    "        lemma_file.write(\"%s\\n\" % item)\n",
    "with open(\"1-2-3_new_story.txt\", \"w\") as story_file:\n",
    "    for item in stories2:\n",
    "        story_file.write(\"%s\\n\" % item)\n",
    "print(len(lemma_stories2))\n",
    "print(len(stories2))\n",
    "print(len(lda_df))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df[-1] = lemma_stories2\n",
    "# lda_df[-2] = lda_df[-1].apply(lambda x: len(x))\n",
    "# lda_df[-3] = lemma_stories # Not a good indicator\n",
    "# lda_df[-4] = lda_df[-3].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lda_df.loc[-1] = topics  # adding a row\n",
    "\n",
    "lda_df.index = lda_df.index + 1  # shifting index\n",
    "\n",
    "lda_df = lda_df.sort_index() # moving up\n",
    "\n",
    "lda_df.to_csv('1-2-3_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
