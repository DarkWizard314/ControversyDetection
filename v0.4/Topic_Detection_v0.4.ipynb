{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Topic Model v0.4\n",
    "Author: Nayeem Aquib\n",
    "Email: nayeemaquib@gmail.com\n",
    "Date: 06/14/2017\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "from crawler import search\n",
    "import re\n",
    "\n",
    "# bash script to get a month's data(lemmas)\n",
    "#subprocess.call(\"bash.sh\")\n",
    "\n",
    "# initialize tokenizer and stopwords\n",
    "en_stop = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as',\n",
    "           'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot',\n",
    "           'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each',\n",
    "           'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\",\n",
    "           \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\",\n",
    "           \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more',\n",
    "           'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought',\n",
    "           'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should',\n",
    "           \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "           'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to',\n",
    "           'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\",\n",
    "           'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",\n",
    "           'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself',\n",
    "           'yourselves', 'apos', 's', 'I', 'will', 'go', 'get', '(', ')', '?', ':', ';', ',', '.', '!', '/', '\"', \"'\", \"...\",\n",
    "           \"``\", \"&apos\", \"&apos;s\", \"&apos;&apos;\", \"-lsb-\", \"-rsb-\", \"-lcb-\", \"-rcb-\", \"-lrb-\", \"-rrb-\", \"O&apos;MALLEY\", \"--\"]\n",
    "\n",
    "stop_chars = ['<', '>']\n",
    "\n",
    "stories = []\n",
    "\n",
    "# get all lemmas between a <story>-</story>-pair:\n",
    "def lemmatization(file):\n",
    "    with open(file) as infile:\n",
    "        for line in infile:\n",
    "            l = line.rstrip()\n",
    "            if l == \"<story>\":\n",
    "                story = []\n",
    "            elif l == \"</story>\":\n",
    "                stories.append(story)\n",
    "                story = []\n",
    "            elif not any(stop_char in l for stop_char in stop_chars):\n",
    "                if l not in en_stop:\n",
    "                    story.append(l)\n",
    "\n",
    "\n",
    "# create dictionary and wordcounts corpus:\n",
    "def buildDictionary():\n",
    "    dictionary = corpora.Dictionary(stories)\n",
    "    dictionary.save(\"wordcounts.dict\")\n",
    "    #print(\"Number of tokens: {}\".format(len(dictionary)))\n",
    "    return dictionary\n",
    "\n",
    "def buildCorpus():\n",
    "    dictionary = buildDictionary()\n",
    "    corpus = [dictionary.doc2bow(story) for story in stories]\n",
    "    corpora.MmCorpus.serialize(\"corpus.mm\", corpus)\n",
    "    #print(\"Length of corpus: {}\".format(len(corpus)))\n",
    "    #print(\"Number of stories: {}\".format(len(stories)))\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# create tf.idf model:\n",
    "def tfidfcorpus():\n",
    "    corpus = buildCorpus()\n",
    "    tfidf_model = models.TfidfModel(corpus)\n",
    "    tfidf_model.save(\"tfidf_model\")\n",
    "    tfidf_corpus = tfidf_model[corpus]\n",
    "    corpora.MmCorpus.serialize(\"tfidf_corpus.mm\", tfidf_corpus)\n",
    "    return tfidf_corpus\n",
    "\n",
    "# create topic model:\n",
    "\n",
    "# LDA\n",
    "def ldamodel(num_topics=50):\n",
    "    lda_model = models.ldamodel.LdaModel(corpus=tfidfcorpus(), id2word=buildDictionary(), num_topics=num_topics, update_every=1,\n",
    "                                         chunksize=5000, passes=50)\n",
    "    lda_model.save(\"lda_model\")\n",
    "    return lda_model\n",
    "\n",
    "def ldacorpus():\n",
    "    lda_corpus = ldamodel(50)[buildCorpus()]\n",
    "    corpora.MmCorpus.serialize(\"lda_corpus.mm\", lda_corpus)\n",
    "    return lda_corpus\n",
    "\n",
    "def findTopicWithURL(topics_found_lda):\n",
    "    counter = 1\n",
    "    for t in topics_found_lda:\n",
    "        print(\"Topic #{} {}\".format(counter, t))\n",
    "        words = re.findall('\"([^\"]+)\"', t[1])\n",
    "        words = ' '.join(words)\n",
    "        print(words)\n",
    "        for url in search(words, stop=3):\n",
    "            with open('out.txt', 'w') as f:\n",
    "            f.write(url)\n",
    "\n",
    "            print(url)\n",
    "        counter += 1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization('1st_January_2016.txt')\n",
    "\n",
    "print(\"\\nTopics by Latent Dirichlet Allocation model\")\n",
    "\n",
    "topics_found_lda = ldamodel().print_topics(num_topics=10, num_words=10)\n",
    "findTopicWithURL(topics_found_lda)\n",
    "\n",
    "lda_df = pd.DataFrame(columns=range(100))\n",
    "\n",
    "for i in range(len(stories)):\n",
    "    doc = ldacorpus()[i]\n",
    "    for top, prob in doc:\n",
    "        lda_df.set_value(i, top, prob)\n",
    "\n",
    "lda_df.to_csv('1st_January_2016.csv')\n",
    "\n",
    "print(\"Doc: {}\".format(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# def findTopic():\n",
    "#     counter = 1\n",
    "#     for t in topics_found_lda:\n",
    "#         print(\"Topic #{} {}\".format(counter, t))\n",
    "#         counter += 1\n",
    "# findTopic()\n",
    "#\n",
    "#\n",
    "# wordcounts_dictionary = buildDictionary()\n",
    "# wordcounts_corpus = buildCorpus()\n",
    "# #tfidf_model = models.TfidfModel(wordcounts_corpus)\n",
    "# tfidf_corpus = tfidfcorpus()\n",
    "# lda_model = ldamodel(100)\n",
    "# lda_corpus = lda_model[tfidf_corpus]\n",
    "#\n",
    "# lda_df = pd.DataFrame(columns=range(100))\n",
    "# for i in range(len(stories)):\n",
    "#     doc = lda_corpus[i]\n",
    "#     for top, prob in doc:\n",
    "#         lda_df.set_value(i, top, prob)\n",
    "#\n",
    "# print(lda_df)\n",
    "# #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "#\n",
    "# top_topics = lda_model.top_topics(corpus, num_words=20)\n",
    "# avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "# print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "#\n",
    "# data_vis_lda = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "# pyLDAvis.display(data_vis_lda)\n",
    "#\n",
    "# print(lda_model.print_topics(num_topics=5, num_words=5))\n",
    "#\n",
    "# #LSI\n",
    "# lsi_model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=10) # initialize an LSI transformation\n",
    "# lsi_corpus = lsi_model[tfidf_corpus]\n",
    "# print(lsi_model.print_topics(5, 5))\n",
    "#\n",
    "# #HDP\n",
    "# hdp_model = models.hdpmodel.HdpModel(corpus, dictionary, T=30)\n",
    "# hdp_model.save(\"hdp_model\")\n",
    "#\n",
    "# print(\"\\nTopics by Hierarchical Dirichlet process model\")\n",
    "# topics_found_hdp = hdp_model.print_topics(num_topics=10, num_words=5)\n",
    "# counter = 1\n",
    "# for t in topics_found_hdp:\n",
    "#     print(\"Topic #{} {}\".format(counter, t))\n",
    "#     counter += 1\n",
    "# vis_hdp = gensimvis.prepare(hdp_model, corpus, dictionary)\n",
    "# pyLDAvis.display(vis_hdp)\n",
    "#\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
